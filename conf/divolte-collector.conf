divolte {
  global {
    server {
      // The host to which the server binds.
      // Set to a specific IP address to selectively listen on that interface.
      host = 0.0.0.0
      host = ${?DIVOLTE_HOST}

      // The port on which the sever listens.
      port = 8290
      port = ${?DIVOLTE_PORT}

      // Whether to use the X-Forwarded-For header HTTP header
      // for determining the source IP of a request if present.
      // When a X-Forwared-For header is present, the rightmost
      // IP address of the value is used as source IP when
      // when multiple IP addresses are separated by a comma.
      // When the header is present more than once, the last
      // value will be used.
      // E.g.
      // "X-Forwarded-For: 10.200.13.28, 11.45.82.30" ==> 11.45.82.30
      //
      // "X-Forwarded-For: 10.200.13.28"
      // "X-Forwarded-For: 11.45.82.30" ==> 11.45.82.30
      use_x_forwarded_for = false
      use_x_forwarded_for = ${?DIVOLTE_USE_XFORWARDED_FOR}

      // When true Divolte Collector serves a static test page at /.
      serve_static_resources = true
      serve_static_resources = ${?DIVOLTE_SERVICE_STATIC_RESOURCES}
    }
    
    mapper {
      // Number of threads to use for processing incoming requests
      threads = 2
      threads = ${?DIVOLTE_INCOMING_REQUEST_PROCESSOR_THREADS}

      // This configures the ip2geo database for geo lookups. A ip2geo database
      // can be obtained from MaxMind (https://www.maxmind.com/en/geoip2-databases).
      // Both a free version and a more accurate paid version are available.
      //
      // By default, no ip2geo database is configured. When this setting is
      // absent, no attempt will be made to lookup geo-coordinates for IP
      // addresses. If configured, Divolte Collector will keep a filesystem
      // watch on the database file. If the file is changed on the filesystem
      // the database will be reloaded at runtime without requireing a restart.
      // ip2geo_database = /path/to/dabase/file.db

      user_agent_parser {
        // The parser type.
        type = non_updating
        type = ${?DIVOLTE_UA_PARSER_TYPE}

        // User agent parsing is a relatively expensive operation that requires
        // many regular expression evaluations. Very often the same user agent
        // will make consecutive requests and many clients will have the exact
        // same user agent as well. It therefore makes sense to cache the
        // parsing results in memory and do a lookup before trying a parse.
        // This setting determines how many unique user agent strings will be
        // cached.
        cache_size = 1000
        cache_size = ${?DIVOLTE_UA_PARSER_CACHE_SIZE}
      }

      // Browsers and other clients (e.g. anti-virus software, proxies)
      // will sometimes send the exact same request twice. Divolte
      // Collector attempts to flag these duplicate events, by using
      // a internal probabilistic data structure with a finite memory
      // size. The memory consists internally of an array of 64 bit
      // integers. This the memory required in bytes is the memory size
      // times 8 (8 megabytes for 1 million entries).
      // Note that the memory size is per thread.
      duplicate_memory_size = 1000000
      duplicate_memory_size = ${?DIVOLTE_INCOMING_REQUEST_PROCESSOR_DUPLICATE_MEMORY_SIZE}
    }
    
    kafka {
      // If true, flushing to Kafka is enabled.
      enabled = true
      enabled = ${?DIVOLTE_KAFKA_ENABLED}

      // Number of threads to use for flushing events to Kafka
      threads = 2
      threads = ${?DIVOLTE_KAFKA_THREADS}

      // The maximum queue of incoming requests to keep
      // before starting to drop incoming requests. Note
      // that when this queue is full, requests are dropped
      // and a warning is logged. No errors are reported to
      // the client side. Divolte Collector will always respond
      // with a HTTP 200 status code and the image payload.
      // Note that the queue size is per thread.
      buffer_size = 1048576
      buffer_size = ${?DIVOLTE_KAFKA_MAX_WRITE_QUEUE}
      
      // All settings in here are used as-is to configure
      // the Kafka producer.
      // See: http://kafka.apache.org/documentation.html#producerconfigs
      producer = {
        bootstrap.servers = ["10.141.0.81:9092"]
        bootstrap.servers = ${?DIVOLTE_KAFKA_BROKER_LIST}

        client.id = divolte.collector
        client.id = ${?DIVOLTE_KAFKA_CLIENT_ID}

        acks = 0
        acks = ${?DIVOLTE_KAFKA_REQUIRED_ACKS}
        retries = 5
        retries = ${?DIVOLTE_KAFKA_MAX_RETRIES}
      }
    }
 
    hdfs {
      // If true, flushing to HDFS is enabled.
      enabled = true
      enabled = ${?DIVOLTE_HDFS_ENABLED}
      
      // Number of threads to use for flushing events to HDFS.
      // Each thread creates its own files on HDFS. Depending
      // on the flushing strategy, multiple concurrent files
      // could be kept open per thread.
      threads = 2
      threads = ${?DIVOLTE_HDFS_THREADS}

      client {
        // default nonexistant: Use HADOOP_CONF_DIR on the classpath.
        // If not present empty config results in local filesystem being used.
        fs.defaultFS = "hdfs://ip:8020/"
        fs.defaultFS = ${?DIVOLTE_HDFS_URI}
      }
		}
  }

  sources {
    browser = {
      type = browser      
    }

    json = {
      type = json
      event_path = /json
    }
  }

  mappings {
      my_mapping = {
        schema_file = "/usr/local/conf/MyEventRecord.avsc"
        mapping_script_file = "/usr/local/conf/mapping.groovy"
        sources = [browser, json]
        sinks = [hdfs,kafka]
      }
  }
  
  sinks {
    kafka = {
      type = kafka

      // The Kafka topic onto which events are published.
      topic = "divolte-data"

      // The topic can be overridden by setting the
      // DIVOLTE_KAFKA_TOPIC environment variable.
      topic = ${?DIVOLTE_KAFKA_TOPIC}
    }

    hdfs = {
      type = hdfs

      // Divolte Collector has two strategies for creating files
      // on HDFS and flushing data. By default, a simple rolling
      // file strategy is empoyed. This opens one file per thread
      // and rolls on to a new file after a configurable interval.
      // Files that are being written to, have a extension of
      // .avro.partial and are written the the directory configured
      // in the working_dir setting. When a file is closed, it
      // will be renamed to have a .avro extension and is moved to
      // the directory configured in the publish_dir settins. This
      // happens in a single (atomic) filesystem move operation.
      file_strategy {
        // Roll over files on HDFS after this amount of time.
        roll_every = 60 minutes
        roll_every = ${?DIVOLTE_HDFS_ROLL_EVERY}

        // Issue a hsync against files each time this number of
        // records has been flushed to it.
        sync_file_after_records = 1000
        sync_file_after_records = ${?DIVOLTE_HDFS_SYNC_FILE_AFTER_RECORDS}

        // If no records are being flushed, issue a hsync when
        // this amount of time passes, regardless of how much
        // data was written.
        sync_file_after_duration = 30 seconds
        sync_file_after_duration = ${?DIVOLTE_HDFS_SYNC_FILE_AFTER_DURATION}

        // Directory where files are created and kept while being
        // written to.
        working_dir = /tmp
        working_dir = ${?DIVOLTE_HDFS_WORKING_DIR}

        // Directory where files are moved to, after they are closed.
        publish_dir = /tmp
        publish_dir = ${?DIVOLTE_HDFS_PUBLISH_DIR}
      }

      // The HDFS replication factor to use when creating
      // files.
      replication = 1
      replication = ${?DIVOLTE_HDFS_REPLICATION}
    }
  }
}